{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "############################################################################################\n",
    "   ###################### Desk Reject Project: Creating the model\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "#################################################\n",
    "  ###### SQL request to get data; then put data in pandas df.\n",
    "#################################################\n",
    "############################\n",
    "  #### Make a connection to the EM data source and execute the query:\n",
    "############################\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "conn = pymssql.connect(server='*WITHHELD*', user='*WITHHELD*', password='*WITHHELD*', database='*WITHHELD*')\n",
    "    # NOTE: database='PONE' is not needed but seems to speed up the query.\n",
    "\n",
    "####### Make a call on the source with a specific SQL querry\n",
    "cur = conn.cursor(as_dict=True)     # 'as_dict' allow you to transform the query into a pandas df\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT d.PUBDNUMBER, d.DOCUMENTID, d.DTITLE, d.SHORT_TITLE, d.ABSTRACT_TEXT, d.ORIGINAL_SUBMISSION_START_DATE,\n",
    "        d.CATEGORY, d.ENTER_KEYWORDS, d.REQUEST_EDITOR, d.SUGGEST_REVIEWERS, d.OPPOSE_REVIEWERS,\n",
    "        a.DEPARTMENT, a.INSTITUTE, a.COUNTRY, a.PTITLE, a.POSITION, a.ALLAUTHORS, \n",
    "        a.ALL_AUTHOR_CONTRIBUTOR_ROLES, a.SECTIONNAME, a.FUNDER_NAME\n",
    "    FROM pone.dbo.DOCUMENT AS d\n",
    "    LEFT JOIN pone.dbo.ROLEAUTH_DOC_PEOPLE_ADDR AS a on d.DOCUMENTID = a.DOCUMENTID\n",
    "    WHERE d.ORIGINAL_SUBMISSION_START_DATE >= '2012-01-10 00:00:00.000' AND d.ORIGINAL_SUBMISSION_START_DATE <= '2020-03-31 00:00:00.000' AND d.PUBDNUMBER IS NOT NULL\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Put the data into a pandas df\n",
    "############################\n",
    "data = []\n",
    "for i in range(9000000):\n",
    "    if cur.fetchone() is None:\n",
    "        break\n",
    "    else:\n",
    "        row = cur.fetchone()\n",
    "        data.append(row)\n",
    "\n",
    "data = pd.DataFrame(data[0:len(data)-1])\n",
    "\n",
    "\n",
    "############################\n",
    "  #### De-duplicate DOCUMENTID, taking only the 1st row\n",
    "############################\n",
    "data = data.drop_duplicates(subset=['DOCUMENTID'], keep= 'first')\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Import the list of MS numbers that are desk rejects and make dv in main data (1 = desk reject 0 = not):\n",
    "############################\n",
    "import numpy as np\n",
    "dv = pd.read_csv(\"All desk rejects since 2012.csv\")\n",
    "data = pd.merge(data, dv[['ManuscriptNumber', 'Date_Submit']], how='left', left_on = ['PUBDNUMBER'], right_on = ['ManuscriptNumber'])\n",
    "data['desk_reject'] = np.where(data[['Date_Submit']].isnull(), 0, 1)\n",
    "#### Check it worked:\n",
    "#data[data['Date_Submit'].notnull()]\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Drop unecessary column from dv:\n",
    "############################\n",
    "data = data.drop(['ManuscriptNumber'], axis=1)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### You can subset the data by year if you'd like here (e.g. compare model for only after 2016):\n",
    "############################\n",
    "################################################################## DISABLED\n",
    "####### Create date only variable from ORIGINAL_SUBMISSION_START_DATE:\n",
    "#from datetime import datetime\n",
    "#temp = []\n",
    "#for i in range(len(data)):\n",
    "#    a = datetime.strptime(data['ORIGINAL_SUBMISSION_START_DATE'][i].split(' ')[0], '%Y-%m-%d')\n",
    "#    temp.append(a)\n",
    "#data['date'] = temp\n",
    "\n",
    "#data = data[data['date'] > '2016-01-01'].reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "  #### Save a copy of your data so you don't have to re-get the SQL:\n",
    "############################\n",
    "saved_copy_of_data = data.copy()\n",
    "\n",
    "############################\n",
    "  #### Load the copy of your saved initial data:\n",
    "############################\n",
    "#data = saved_copy_of_data\n",
    "\n",
    "############################\n",
    "  #### Subset the data by year (e.g. compare model for only after 2017):\n",
    "############################\n",
    "####### Create date only variable from ORIGINAL_SUBMISSION_START_DATE:\n",
    "#from datetime import datetime\n",
    "#temp = []\n",
    "#for i in range(len(data)):\n",
    "#    a = datetime.strptime(str(data['ORIGINAL_SUBMISSION_START_DATE'][i]).split(' ')[0], '%Y-%m-%d')\n",
    "#    temp.append(a)\n",
    "#data['date'] = temp\n",
    "\n",
    "#data = data[data['date'] > '2017-01-01'].reset_index(drop = True)\n",
    "#data = data[data['date'] < '2020-04-01'].reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and then parse down free-text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### USING COUNT VECTOR TO CREATE WORD/BI & TRI GRAM FREQUENCIES:\n",
    "    # NOTE: Counts are better than TF-IDFs in this case: Same model quality metrics but tf-idfs require re-uploading all other values, word counts do not\n",
    "       # See supplmental code: tf-idfs may be better for other projects.\n",
    "#################################################\n",
    "#####################\n",
    " # Create a sparse matrix of all the tf-idfs. Variable by variable for the following variables:\n",
    "  ## DTITLE\n",
    "  ## SHORT_TITLE\n",
    "  ## ABSTRACT_TEXT\n",
    "  ## DEPARTMENT\n",
    "  ## INSTITUTE\n",
    "  ## PTITLE\n",
    "  ## ALL_AUTHOR_CONTRIBUTOR_ROLES\n",
    "  ## FUNDER_NAME\n",
    "#####################\n",
    "###############\n",
    " # The following is a count vectorizer: This gives a frequency of words, bigrams and trigrams (with ngram_range as 1,3)\n",
    "    # ALSO: a word or bi-gram or tri-gram in less than 2 docs or more than 80% of docs is removed).\n",
    "    # INCLUDES stopwords\n",
    "###############\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix = vect.fit_transform(data['DTITLE'].astype(str))\n",
    "vect_2 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_2 = vect_2.fit_transform(data['SHORT_TITLE'].astype(str))\n",
    "vect_3 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_3 = vect_3.fit_transform(data['ABSTRACT_TEXT'].astype(str))\n",
    "vect_4 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_4 = vect_4.fit_transform(data['DEPARTMENT'].astype(str))\n",
    "vect_5 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_5 = vect_5.fit_transform(data['INSTITUTE'].astype(str))\n",
    "vect_6 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_6 = vect_6.fit_transform(data['PTITLE'].astype(str))\n",
    "vect_7 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_7 = vect_7.fit_transform(data['ALL_AUTHOR_CONTRIBUTOR_ROLES'].astype(str))\n",
    "vect_8 = CountVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 3))\n",
    "doc_term_matrix_8 = vect_8.fit_transform(data['FUNDER_NAME'].astype(str))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create non-free text variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### Create variables: Make into dummy variables\n",
    "#################################################\n",
    "############################\n",
    "  #### Not_filled then drop:\n",
    "############################\n",
    "####### POSITION\n",
    "import numpy as np\n",
    "data['POSITION_not_filled'] = np.where((data['POSITION'].isnull()) | (data['POSITION'] == \" \") | (data['POSITION'] == \"\"), 1, 0)\n",
    "data = data.drop('POSITION', axis=1)\n",
    "####### FUNDER_NAME: Turn into 'anything filled in vs. not'\n",
    "data['FUNDER_NAME_not_filled'] = np.where((data['FUNDER_NAME'].isnull()) | (data['FUNDER_NAME'] == \" \") | (data['FUNDER_NAME'] == \"\"), 1, 0)\n",
    "\n",
    "data = data.drop('FUNDER_NAME', axis=1)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### CATEGORY: DUE TO LOW NUMBERS ON MANY OF THEM, I ONLY USED:\n",
    "      # 'Research Article'\n",
    "      # 'Research Articles'\n",
    "      # Either 'Research Article' or 'Research Articles'\n",
    "############################\n",
    "data['CATEGORY_clinical_trial'] = np.where((data['CATEGORY'] == 'Clinical Trial') | (data['CATEGORY'] == 'Clinical trial'), 1, 0)\n",
    "data['CATEGORY_Research_Article'] = np.where((data['CATEGORY'] == 'Research Article'), 1, 0)\n",
    "data['CATEGORY_Research_Articles'] = np.where((data['CATEGORY'] == 'Research Articles'), 1, 0)\n",
    "data['CATEGORY_RA_or_RAs'] = np.where((data['CATEGORY'] == 'Research Article') | (data['CATEGORY'] == 'Research Articles'), 1, 0)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### SECTIONNAME: Create 20 dummy variables OR create specific ones; Also create a 'not_filled'\n",
    "############################\n",
    "data['SECTIONNAME_not_filled'] = np.where((data['SECTIONNAME'].isnull()) | (data['SECTIONNAME'] == \" \") | (data['SECTIONNAME'] == \"\"), 1, 0)\n",
    "#data = pd.get_dummies(data, columns=['SECTIONNAME'])\n",
    "\n",
    "####### Only using specific SECTIONNAMES with higher/lower importance:\n",
    "data['SECTIONNAME_Life & Social Sciences'] = np.where((data['SECTIONNAME'] == 'Life & Social Sciences'), 1, 0)\n",
    "data['SECTIONNAME_Life Sciences'] = np.where((data['SECTIONNAME'] == 'Life Sciences'), 1, 0)\n",
    "data['SECTIONNAME_Other'] = np.where((data['SECTIONNAME'] == 'Other'), 1, 0)\n",
    "data['SECTIONNAME_Medicine and Health Sciences'] = np.where((data['SECTIONNAME'] == 'Medicine and Health Sciences'), 1, 0)\n",
    "data['SECTIONNAME_SECTIONNAME_Medicine and public health'] = np.where((data['SECTIONNAME'] == 'SECTIONNAME_Medicine and public health'), 1, 0)\n",
    "data['SECTIONNAME_Environmental Sciences'] = np.where((data['SECTIONNAME'] == 'Environmental Sciences'), 1, 0)\n",
    "data['SECTIONNAME_Clinical'] = np.where((data['SECTIONNAME'] == 'Clinical'), 1, 0)\n",
    "data['SECTIONNAME_Physical sciences and engineering'] = np.where((data['SECTIONNAME'] == 'Physical sciences and engineering'), 1, 0)\n",
    "data['SECTIONNAME_Clinical Sciences'] = np.where((data['SECTIONNAME'] == 'Clinical Sciences'), 1, 0)\n",
    "data['SECTIONNAME_Applied Mathematics'] = np.where((data['SECTIONNAME'] == 'Applied Mathematics'), 1, 0)\n",
    "data['SECTIONNAME_Earth Sciences'] = np.where((data['SECTIONNAME'] == 'Earth Sciences'), 1, 0)\n",
    "data['SECTIONNAME_Social and Behavioral Sciences'] = np.where((data['SECTIONNAME'] == 'Social and Behavioral Sciences'), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "  #### COUNTRY: 'anything filled in vs. not' ... Dummy variables for these: PAKISTAN, ETHIOPIA, INDIA, CHINA, UNITED STATES\n",
    "    # NOTE: You could also try out EVERY country: just use the disabled code\n",
    "############################\n",
    "####### Using ALL countries as separate dummy variables:\n",
    "#data['COUNTRY_not_filled'] = np.where(data[['COUNTRY']].isnull(), 1, 0)\n",
    "#data = pd.get_dummies(data, columns=['COUNTRY'])\n",
    "\n",
    "####### Only using specific countries with higher/lower importance::\n",
    "data['COUNTRY_not_filled'] = np.where(data[['COUNTRY']].isnull(), 1, 0)\n",
    "\n",
    "data['COUNTRY_AUSTRALIA'] = np.where((data['COUNTRY'] == 'AUSTRALIA'), 1, 0)\n",
    "data['COUNTRY_BANGLADESH'] = np.where((data['COUNTRY'] == 'BANGLADESH'), 1, 0)\n",
    "data['COUNTRY_CANADA'] = np.where((data['COUNTRY'] == 'CANADA'), 1, 0)\n",
    "data['COUNTRY_CHINA'] = np.where((data['COUNTRY'] == 'CHINA'), 1, 0)\n",
    "data['COUNTRY_EGYPT'] = np.where((data['COUNTRY'] == 'EGYPT'), 1, 0)\n",
    "data['COUNTRY_ETHIOPIA'] = np.where((data['COUNTRY'] == 'ETHIOPIA'), 1, 0)\n",
    "data['COUNTRY_GERMANY'] = np.where((data['COUNTRY'] == 'GERMANY'), 1, 0)\n",
    "data['COUNTRY_INDIA'] = np.where((data['COUNTRY'] == 'INDIA'), 1, 0)\n",
    "data['COUNTRY_IRAN, ISLAMIC REPUBLIC OF'] = np.where((data['COUNTRY'] == 'IRAN, ISLAMIC REPUBLIC OF'), 1, 0)\n",
    "data['COUNTRY_MALAYSIA'] = np.where((data['COUNTRY'] == 'MALAYSIA'), 1, 0)\n",
    "data['COUNTRY_NETHERLANDS'] = np.where((data['COUNTRY'] == 'NETHERLANDS'), 1, 0)\n",
    "data['COUNTRY_NIGERIA'] = np.where((data['COUNTRY'] == 'NIGERIA'), 1, 0)\n",
    "data['COUNTRY_PAKISTAN'] = np.where((data['COUNTRY'] == 'PAKISTAN'), 1, 0)\n",
    "data['COUNTRY_SAUDI ARABIA'] = np.where((data['COUNTRY'] == 'SAUDI ARABIA'), 1, 0)\n",
    "data['COUNTRY_UNITED KINGDOM'] = np.where((data['COUNTRY'] == 'UNITED KINGDOM'), 1, 0)\n",
    "data['COUNTRY_UNITED STATES'] = np.where((data['COUNTRY'] == 'UNITED STATES'), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "  ###### Create variables: Varied:\n",
    "#################################################\n",
    "############################\n",
    "  #### DEPARTMENT: (a) Has only one word (b) not filled in\n",
    "############################\n",
    "### One word:\n",
    "f = data[data['DEPARTMENT'].astype(str).str.contains(\" \")]\n",
    "data['DEPARTMENT_over_one_word'] = np.where(data.index.isin(f.index), 1, 0)\n",
    "### Not filled in:\n",
    "data['DEPARTMENT_not_filled'] = np.where((data['DEPARTMENT'].isnull()) | (data['DEPARTMENT'] == \"\"), 1, 0)\n",
    "\n",
    "####### Create a ratio variable of the number of Characters (ratio):\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = len(str(data['DEPARTMENT'][i]))\n",
    "    temp.append(a)\n",
    "data['DEPARTMENT_num_characters'] = temp\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "  #### ALLAUTHORS: Count the number of semicolons + 1. Use as ratio but also turn into a binned (dummy) variable\n",
    "############################\n",
    "####### Create a ratio variable of the number of authors (minus 1):\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = str(data['ALLAUTHORS'][i]).count(';')\n",
    "    temp.append(a)\n",
    "data['ALLAUTHORS_ratio_count'] = temp\n",
    "\n",
    "####### Some have a ton of authors so bin instead: 1; 2-3; 4-5; 6-7; 8-10; 11-14; 15-20; 21-35; 35+\n",
    "############# ONLY USE 2-3 AND RATIO\n",
    "data['ALLAUTHORS_1'] = np.where((data['ALLAUTHORS_ratio_count'] == 1), 1, 0)\n",
    "data['ALLAUTHORS_2_3'] = np.where((data['ALLAUTHORS_ratio_count'] == 2) | (data['ALLAUTHORS_ratio_count'] == 3), 1, 0)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### INSTITUTE: Anything filled in vs. not' THEN number of characters (ratio) THEN number of words (ratio) THEN Words over 4 THEN words over 6\n",
    "############################\n",
    "####### Create variable: INSTITUTE not filled in:\n",
    "data['INSTITUTE_not_filled'] = np.where((data['INSTITUTE'].isnull()) | (data['INSTITUTE'] == \" \") | (data['INSTITUTE'] == \"\"), 1, 0)\n",
    "\n",
    "####### Create a ratio variable of the number of Characters (ratio):\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = len(str(data['INSTITUTE'][i]))\n",
    "    temp.append(a)\n",
    "data['INSTITUTE_num_characters'] = temp\n",
    "\n",
    "\n",
    "############################\n",
    "  #### ORIGINAL_SUBMISSION_START_DATE: Dummy variables for each day of the week; Dummy variable for each month:\n",
    "############################\n",
    "####### Create date only variable from ORIGINAL_SUBMISSION_START_DATE:\n",
    "from datetime import datetime\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = datetime.strptime(str(data['ORIGINAL_SUBMISSION_START_DATE'][i]).split(' ')[0], '%Y-%m-%d')\n",
    "    temp.append(a)\n",
    "data['date'] = temp\n",
    "data.tail()\n",
    "\n",
    "####### Create variables for each day of the week:\n",
    "  # NOTE: Monday is 0 and Sunday is 6.\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = data['date'][i].weekday()\n",
    "    temp.append(a)\n",
    "data['dayofweek'] = temp\n",
    "#### Make dummy variables:\n",
    "data['dayofweek_mon'] = np.where((data['dayofweek'] == 0), 1, 0)\n",
    "data['dayofweek_tue'] = np.where((data['dayofweek'] == 1), 1, 0)\n",
    "data['dayofweek_wed'] = np.where((data['dayofweek'] == 2), 1, 0)\n",
    "data['dayofweek_thur'] = np.where((data['dayofweek'] == 3), 1, 0)\n",
    "data['dayofweek_fri'] = np.where((data['dayofweek'] == 4), 1, 0)\n",
    "data['dayofweek_sat'] = np.where((data['dayofweek'] == 5), 1, 0)\n",
    "data['dayofweek_sun'] = np.where((data['dayofweek'] == 6), 1, 0)\n",
    "\n",
    "data = data.drop('dayofweek', axis=1)\n",
    "\n",
    "\n",
    "####### Create dummy variables for each month:\n",
    "temp = []\n",
    "for i in range(len(data)):\n",
    "    a = data['date'][i].month\n",
    "    temp.append(a)\n",
    "data['month'] = temp\n",
    "\n",
    "data['month_1'] = np.where((data['month'] == 1), 1, 0)\n",
    "data['month_2'] = np.where((data['month'] == 2), 1, 0)\n",
    "data['month_3'] = np.where((data['month'] == 3), 1, 0)\n",
    "data['month_4'] = np.where((data['month'] == 4), 1, 0)\n",
    "data['month_5'] = np.where((data['month'] == 5), 1, 0)\n",
    "data['month_6'] = np.where((data['month'] == 6), 1, 0)\n",
    "data['month_7'] = np.where((data['month'] == 7), 1, 0)\n",
    "data['month_8'] = np.where((data['month'] == 8), 1, 0)\n",
    "data['month_9'] = np.where((data['month'] == 9), 1, 0)\n",
    "data['month_10'] = np.where((data['month'] == 10), 1, 0)\n",
    "data['month_11'] = np.where((data['month'] == 11), 1, 0)\n",
    "data['month_12'] = np.where((data['month'] == 12), 1, 0)\n",
    "\n",
    "data = data.drop('month', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "  ###### Combine pandas data of non-free text into the main I.V. list: the 'X_list' sparse matrix)\n",
    "#################################################\n",
    "############################\n",
    "  #### Accumulate all pandas columns to include in analysis:\n",
    "############################\n",
    "####### Make sure to keep these 2 pre-created variables:\n",
    "data['REQUEST_EDITOR_2'] = np.where((data['REQUEST_EDITOR'] == 1), 1, 0)\n",
    "data['OPPOSE_REVIEWERS_2'] = np.where((data['OPPOSE_REVIEWERS'] == 1), 1, 0)\n",
    "\n",
    "##### The 1st variable to include is \"POSITION_not_filled\", get that position until the end\n",
    "   # Remove 'date' column too\n",
    "data = data.drop(['date'], axis=1)\n",
    "start_col = data.columns.get_loc('POSITION_not_filled')\n",
    "for_x_list = data.iloc[:, start_col : len(data.columns)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### Combing ALL free text words and ngrams with the non-free text variables\n",
    "#################################################\n",
    "#####################\n",
    " # Add the sparse matracies together as one variable list: https://stackoverflow.com/questions/19710602/concatenate-sparse-matrices-in-python-using-scipy-numpy\n",
    "     # This 'X_list' will be you main list of all independent variables\n",
    "#####################\n",
    "####### Combining the sparse matricies:\n",
    "from scipy.sparse import hstack\n",
    "X_list = hstack((doc_term_matrix, doc_term_matrix_2, doc_term_matrix_3 ,doc_term_matrix_4, doc_term_matrix_5, doc_term_matrix_6, doc_term_matrix_7, doc_term_matrix_8))\n",
    "\n",
    "\n",
    "#####################\n",
    " # Combine the feature names AND which variable they come from so you know what is what:\n",
    "#####################\n",
    "aa = pd.DataFrame(vect.get_feature_names())\n",
    "aa['main_var'] = 'DTITLE'\n",
    "bb = pd.DataFrame(vect_2.get_feature_names())\n",
    "bb['main_var'] = 'SHORT_TITLE'\n",
    "cc = pd.DataFrame(vect_3.get_feature_names())\n",
    "cc['main_var'] = 'ABSTRACT_TEXT'\n",
    "dd = pd.DataFrame(vect_4.get_feature_names())\n",
    "dd['main_var'] = 'DEPARTMENT'\n",
    "ee = pd.DataFrame(vect_5.get_feature_names())\n",
    "ee['main_var'] = 'INSTITUTE'\n",
    "ff = pd.DataFrame(vect_6.get_feature_names())\n",
    "ff['main_var'] = 'PTITLE'\n",
    "gg = pd.DataFrame(vect_7.get_feature_names())\n",
    "gg['main_var'] = 'ALL_AUTHOR_CONTRIBUTOR_ROLES'\n",
    "hh = pd.DataFrame(vect_8.get_feature_names())\n",
    "hh['main_var'] = 'FUNDER_NAME'\n",
    "\n",
    "####### Combining all the feature names\n",
    "full_features = pd.concat([aa, bb, cc, dd, ee, ff, gg, hh], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Add pandas columns into the sparse matrix:\n",
    "############################\n",
    "X_list = hstack((X_list, np.array(for_x_list)))\n",
    "   # NOTE: category must be an integer not a float.\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Add on the pandas column names to the 'full_features' list of feature names\n",
    "############################\n",
    "for_x_list = pd.DataFrame(list(for_x_list.columns))\n",
    "for_x_list['main_var'] = 'non_text'\n",
    "full_features = pd.concat([pd.DataFrame(full_features), for_x_list], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells:\n",
    "  ## A: Create a test/training data and uses SMOTE to correct for imbalanced classification\n",
    "  ## B: Fit a RandomForrest or GradientBoostingClassifier model\n",
    "  ## C: Print out model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "############################################################################################\n",
    "   #################### Desk Reject Project: Modelling 'desk_rejects' with decision trees\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "#################################################\n",
    "  ###### Set up the test/training data:\n",
    "#################################################\n",
    "############################\n",
    "  #### Set up the test/training data:\n",
    "############################\n",
    "####### Separate the DV (desk_reject) and all other variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = pd.Series(data['desk_reject'].astype('int64'))\n",
    "X = X_list.copy()\n",
    "\n",
    "####### Create the test and train data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "  #### To do train/test/validaion, do a 2nd split: (DISABLED FOR NOW)\n",
    "############################\n",
    "### If the first test/train is 0.20, then the following would be: 60% train, 20% test, 20% validation:\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "  ###### Deal with imbalenced classification: SMOTE\n",
    "#################################################\n",
    "############################\n",
    "  #### Deal with imbalenced classification: SMOTE\n",
    "    # See the following for examples of SMOTE: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "    # Either use this or pure duplication method.\n",
    "    ## NOTE: Only use SMOTE on the TRAINING data, not the test data. e.g.: https://datascience.stackexchange.com/questions/47228/using-smote-for-synthetic-data-generation-to-improve-performance-on-unbalanced-d\n",
    "  #### NOTE: The other options is pure duplication: just duplicate the desk reject == 1 rows until it's even\n",
    "############################\n",
    "####### Sampling on desk_reject == 1 to balance classes:\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### Different types of classifiers to use:\n",
    "    # Probably use RandomForestClassifier but check the other models to see what works best for your purposes\n",
    "#################################################\n",
    "############################\n",
    "  #### Using RandomForestClassifier\n",
    "############################\n",
    "####### Create the classifier:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#clf=RandomForestClassifier(n_estimators=500) #TRY: max_features = (len(X.columns)-1)\n",
    "    # NOTE: Due to SMOTE, class_weight = 'balanced' not needed\n",
    "#clf.fit(X_train,y_train)\n",
    "####### Get predicted values:\n",
    "#y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Using: GradientBoostingClassifier\n",
    "    # NOTE: This one can take a very long time (over a day)\n",
    "############################\n",
    "####### Create the classifier:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#clf=GradientBoostingClassifier()\n",
    "    ### NOTE: Automatically balances the classes: e.g. answer 2 here: https://stackoverflow.com/questions/35539937/is-there-class-weight-or-alternative-way-for-gradientboostingclassifier-in-skl\n",
    "#clf.fit(X_train,y_train)\n",
    "####### Get predicted values:\n",
    "#y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "############################\n",
    "  #### Using: GradientBoostingClassifier WITH xgboost:\n",
    "    # NOTE: Explaination of xgboost: https://towardsdatascience.com/the-intuition-behind-gradient-boosting-xgboost-6d5eac844920  \n",
    "  #### Parameters to try out ... e.g see: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "    # n_estimators increase the number of estimators from default of 100. Use higher number when you have a ton of features (250/1000/5000 etc)\n",
    "    # parameters to try: \n",
    "      # eta default is 0.3 (try 0.1 or 0.2): Lower preforms worse\n",
    "      # min_child_weight default is 1 (try 2 or 3): Neither is better, just use default.\n",
    "      # max_depth default is 6 (try 5 or 7/8/9)\n",
    "      # gamma default is 0 (try 0.1/0.2): lower performs worse\n",
    "      # subsample default is 1 (try 0.6 or 0.8) Lower preforms worse\n",
    "      # colsample_bytree default is 1 (try 0.6 or 0.8)    ************************* TRY THIS AGAIN (0.8 PERFORMED BETTER)\n",
    "############################\n",
    "####### Create the classifier:\n",
    "from xgboost import XGBClassifier\n",
    "#clf = XGBClassifier()\n",
    "clf = XGBClassifier(colsample_bytree = 0.8)    # Using all the defaults and colsample_bytree = 0.8 turned out to be the best.\n",
    "    ### n_estimators increase the number of estimators from default of 100 due to large number of features.\n",
    "    ### NOTE: Automatically balances the classes: e.g. answer 2 here: https://stackoverflow.com/questions/35539937/is-there-class-weight-or-alternative-way-for-gradientboostingclassifier-in-skl\n",
    "clf.fit(X_train,y_train)\n",
    "####### Get predicted values:\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "  ###### Do several types of accuracy checks:\n",
    "#################################################\n",
    " # precision: lower means it's more likely to get guess something is 0 and then that guess will be wrong.\n",
    " # recall: lower means it's more likely to NOT guess something is 0 when it actually is 0.\n",
    "    # So the recall the desk rejects is quite low.\n",
    " # f-1 score is an overall score of precision and recall.\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "print(classification_report(y_test, y_pred), \"\\n\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### Examine feature importance scores:\n",
    "#################################################\n",
    "####### Print out the features scores in order of score and their column position\n",
    "a = pd.DataFrame(clf.feature_importances_)\n",
    "a.rename(columns={ a.columns[0]: \"feature_score\" }, inplace = True)\n",
    "#a['variable_names'] = list(X.columns)\n",
    "a = a.sort_values(by='feature_score', ascending=False)\n",
    "print(a[0:30])\n",
    "a[0:250].to_csv(r\"feature_importance_raw.csv\")                                          # NEW (remove)\n",
    "\n",
    "####### Print out the names of the TF-IDFs:\n",
    "to_save = []\n",
    "for i in list(a.index[0:250]):\n",
    "    print(full_features['main_var'][i], \" :    \", full_features[0][i])\n",
    "    aa = full_features['main_var'][i], \" :    \", full_features[0][i]\n",
    "    to_save.append(aa)   # NEW (remove)\n",
    "to_save = pd.DataFrame(to_save)                                                         # NEW (remove)\n",
    "to_save.to_csv(r\"feature_importance_var_names.csv\")                                     # NEW (remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ###### Analyze accuracy metrics after changing the threshold of your predictions:\n",
    "     ## NOTE: If you go with a different threshold, you'll need to set that up in the associeted new predictions notebook\n",
    "     ## NOTE: Below doesn't work for decision trees, works for RandomForest and GradientBoostingClassifier\n",
    "  ###### MAIN: Changing threshold can help increase recall of desk rejects, though will decrease precision.\n",
    "     # Depends on business question: do you want to flag potential desk rejects or reject papers that are for sure to be rejected.\n",
    "#################################################\n",
    "\n",
    "####### Change around the threshold to optimize the f1-score of 1:\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "fone = {}\n",
    "for i in range(3, 75):\n",
    "    threshold = i / 100  # use this to optimize\n",
    "    predicted_proba = clf.predict_proba(X_test)\n",
    "    y_pred = (predicted_proba [:,1] >= threshold).astype('int')\n",
    "    fone[i] = round(f1_score(y_test, y_pred),3), round(recall_score(y_test, y_pred),3), round(precision_score(y_test, y_pred),3), round(accuracy_score(y_test, y_pred),3)\n",
    "\n",
    "####### Print out scores of thresholds:\n",
    "diff_thresh = sorted(fone.items(), key=lambda x: x[1], reverse = True)\n",
    "print(\"Threshold,  f1-Score,  Recall,  Precision score, Model Accuracy     ...  Below are sorted by f1 score\")\n",
    "diff_thresh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### This prints out the confusion matrix for different thresholds:\n",
    "predicted_proba = clf.predict_proba(X_test)\n",
    "y_p = (predicted_proba [:,1] >= 0.70).astype('int')\n",
    "print(confusion_matrix(y_test, y_p), \"\\n\")\n",
    "\n",
    "print(\"Percent tagged as desk reject:\", sum(y_p) / len(y_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
